{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1299795,"sourceType":"datasetVersion","datasetId":751906},{"sourceId":2039896,"sourceType":"datasetVersion","datasetId":1221832},{"sourceId":2089321,"sourceType":"datasetVersion","datasetId":1252728}],"dockerImageVersionId":30068,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem definiton\n**Segmentation of gliomas in pre-operative MRI scans.**\n\n*Each pixel on image must be labeled:*\n* Pixel is part of a tumor area (1 or 2 or 3) -> can be one of multiple classes / sub-regions\n* Anything else -> pixel is not on a tumor region (0)\n\nThe sub-regions of tumor considered for evaluation are: 1) the \"enhancing tumor\" (ET), 2) the \"tumor core\" (TC), and 3) the \"whole tumor\" (WT)\nThe provided segmentation labels have values of 1 for NCR & NET, 2 for ED, 4 for ET, and 0 for everything else.\n\n","metadata":{}},{"cell_type":"markdown","source":"# ![Brats official annotations](https://www.med.upenn.edu/cbica/assets/user-content/images/BraTS/brats-tumor-subregions.jpg)","metadata":{}},{"cell_type":"markdown","source":"# Setup env","metadata":{}},{"cell_type":"markdown","source":"# MultiModal Fusion with Convolutional Variational AutoEncoder from BraTS20 dataset\n### modals used=4","metadata":{}},{"cell_type":"markdown","source":"# Calculation of Parameters for Each Layer in the Encoder\n\nLet me break down the parameter calculations for each layer in the encoder model:\n\n## conv3d_20 (Conv3D): 6,976 parameters\n- Input shape: (240, 240, 48, 4) - A 3D volume with 4 channels\n- Filter size: Likely 3×3×3 with 64 output filters\n- Parameters = (3×3×3×4 + 1) × 64 = (108 + 1) × 64 = 109 × 64 = 6,976\n  - 3×3×3: Filter dimensions\n  - 4: Input channels\n  - +1: Bias for each filter\n  - ×64: Number of filters\n\n## max_pooling3d_15 (MaxPooling3D): 0 parameters\n- Pooling operations don't have trainable parameters\n\n## batch_normalization_30 (BatchNorm): 256 parameters\n- Normalizes 64 feature maps\n- Parameters = 64 × 4 = 256\n  - 2 learned parameters per channel (scale and shift)\n  - 2 running statistics per channel (mean and variance)\n\n## conv3d_21 (Conv3D): 221,312 parameters\n- Input: 64 channels from previous layer\n- Filter size: 3×3×3 with 128 output filters\n- Parameters = (3×3×3×64 + 1) × 128 = (1,728 + 1) × 128 = 1,729 × 128 = 221,312\n\n## max_pooling3d_16 (MaxPooling3D): 0 parameters\n- No trainable parameters\n\n## batch_normalization_31 (BatchNorm): 512 parameters\n- Normalizes 128 feature maps\n- Parameters = 128 × 4 = 512\n\n## conv3d_22 (Conv3D): 884,992 parameters\n- Input: 128 channels\n- Filter size: 3×3×3 with 256 output filters\n- Parameters = (3×3×3×128 + 1) × 256 = (3,456 + 1) × 256 = 3,457 × 256 = 884,992\n\n## max_pooling3d_17 (MaxPooling3D): 0 parameters\n- No trainable parameters\n\n## batch_normalization_32 (BatchNorm): 1,024 parameters\n- Normalizes 256 feature maps\n- Parameters = 256 × 4 = 1,024\n\n## global_average_pooling3d_5 (GlobalAveragePooling3D): 0 parameters\n- Pooling operations don't have trainable parameters\n\n## z_mean (Dense): 65,792 parameters\n- Input: 256 features from global pooling\n- Output: 256 latent dimensions\n- Parameters = 256 × 256 + 256 = 65,792\n  - 256 × 256: Weight matrix\n  - 256: Bias terms\n\n## z_log_var (Dense): 65,792 parameters\n- Input: 256 features\n- Output: 256 latent dimensions\n- Parameters = 256 × 256 + 256 = 65,792\n\n## sampling_5 (Sampling): 0 parameters\n- Custom layer that performs the reparameterization trick\n- No trainable parameters\n\n## Total Parameters\n- Sum of all parameters = 6,976 + 0 + 256 + 221,312 + 0 + 512 + 884,992 + 0 + 1,024 + 0 + 65,792 + 65,792 + 0 = 1,246,656\n- Trainable: 1,245,760 (all except some batch norm statistics)\n- Non-trainable: 896 (running statistics in batch normalization layers)\n\nThis parameter distribution shows a typical CNN pattern where the convolutional layers contain the bulk of the parameters, especially as channel depth increases through the network.\n\n# Decoder Parameter Calculations\n\nLet's break down the parameter calculations for each layer in the decoder:\n\n1. **Dense Layer**: `layers.Dense(15*15*6*256, activation='relu')(latent_inputs)`\n   - Input: 256 (latent_dim)\n   - Output: 15×15×6×256 = 345,600 neurons\n   - Parameters = (256 × 345,600) + 345,600 = 88,819,200\n     - 256 × 345,600: Weight matrix\n     - 345,600: Bias terms\n   - This is by far the largest parameter consumer in the entire model\n\n2. **Reshape Layer**: No parameters, just reorganizes the data\n\n3. **Conv3DTranspose (256 filters)**:\n   - Input shape: (15, 15, 6, 256)\n   - Filter size: 3×3×3 with 256 output filters\n   - Parameters = (3×3×3×256 + 1) × 256 = (6,912 + 1) × 256 = 1,769,728\n\n4. **BatchNormalization after first Conv3DTranspose**:\n   - 256 feature maps × 4 parameters per map = 1,024 parameters\n   - Trainable: 512 (gamma, beta)\n   - Non-trainable: 512 (running mean, variance)\n\n5. **Conv3DTranspose (128 filters)**:\n   - Input: 256 channels\n   - Output: 128 filters\n   - Parameters = (3×3×3×256 + 1) × 128 = (6,912 + 1) × 128 = 884,864\n\n6. **BatchNormalization after second Conv3DTranspose**:\n   - 128 feature maps × 4 parameters = 512 parameters\n   - Trainable: 256\n   - Non-trainable: 256\n\n7. **Conv3DTranspose (64 filters)**:\n   - Input: 128 channels\n   - Output: 64 filters\n   - Parameters = (3×3×3×128 + 1) × 64 = (3,456 + 1) × 64 = 221,248\n\n8. **BatchNormalization after third Conv3DTranspose**:\n   - 64 feature maps × 4 parameters = 256 parameters\n   - Trainable: 128\n   - Non-trainable: 128\n\n9. **Conv3DTranspose (32 filters)**:\n   - Input: 64 channels\n   - Output: 32 filters\n   - Parameters = (3×3×3×64 + 1) × 32 = (1,728 + 1) × 32 = 55,328\n\n10. **Final Conv3D (4 filters)**:\n    - Input: 32 channels\n    - Output: 4 channels\n    - Parameters = (3×3×3×32 + 1) × 4 = (864 + 1) × 4 = 3,460\n\nTotal decoder parameters = 91,755,620 (as shown in the model summary)\n\n## Why Sigmoid Activation in the Output Layer?\n\nThe sigmoid activation function in the output layer serves several important purposes:\n\n1. **Range Constraint**: Sigmoid outputs values between 0 and 1, which matches the range of the input data after min-max normalization. The preprocessing step normalized all MRI values to [0,1], so the output needs to produce values in the same range.\n\n2. **Intensity Reconstruction**: In medical imaging, pixel/voxel intensities represent physical properties. The sigmoid function ensures that reconstructed intensities remain within a valid range without clipping.\n\n3. **Probabilistic Interpretation**: In some contexts, output values can be interpreted as probabilities or confidence levels of voxel intensity, which the sigmoid naturally provides.\n\n4. **Smooth Gradients**: Sigmoid provides smooth gradients that are beneficial during training, especially for reconstruction tasks.\n\nAn alternative could have been tanh (outputs -1 to 1) with appropriate rescaling, but since the input data is normalized to [0,1], sigmoid is the natural choice that directly matches this range.\n\nThe dtype specification of 'float32' ensures that even when using mixed precision training, the output maintains full 32-bit precision, which is important for accurate reconstruction of medical images where small intensity differences can be clinically significant.","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport glob\nimport PIL\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom skimage import data\nfrom skimage.util import montage \nimport skimage.transform as skTrans\nfrom skimage.transform import rotate\nfrom skimage.transform import resize\nfrom PIL import Image, ImageOps  \n\n\n# neural imaging\nimport nilearn as nl\nimport nibabel as nib\nimport nilearn.plotting as nlplt\n!pip install git+https://github.com/miykael/gif_your_nifti # nifti to gif \nimport gif_your_nifti.core as gif2nif\n\n\n# ml libs\nimport keras\nimport keras.backend as K\nfrom keras.callbacks import CSVLogger\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\nfrom tensorflow.keras.layers.experimental import preprocessing\n\n\n# Make numpy printouts easier to read.\nnp.set_printoptions(precision=3, suppress=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:04:56.741584Z","iopub.execute_input":"2025-04-21T10:04:56.741917Z","iopub.status.idle":"2025-04-21T10:05:16.291025Z","shell.execute_reply.started":"2025-04-21T10:04:56.741848Z","shell.execute_reply":"2025-04-21T10:05:16.290340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_DATASET_PATH = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/'\nVALIDATION_DATASET_PATH = '../inp|ut/brats20-dataset-training-validation/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:05:30.048894Z","iopub.execute_input":"2025-04-21T10:05:30.049270Z","iopub.status.idle":"2025-04-21T10:05:30.053218Z","shell.execute_reply.started":"2025-04-21T10:05:30.049237Z","shell.execute_reply":"2025-04-21T10:05:30.052131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Parameters\nVOLUME_SLICES = 96\nVOLUME_START_AT = 22\nINPUT_SHAPE = (240, 240, VOLUME_SLICES, 4)\nlatent_dim = 256\nbatch_size = 1\n\n# Sampling layer with explicit dtype handling\nclass Sampling(layers.Layer):\n    def call(self, inputs):\n        z_mean, z_log_var = inputs\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        \n        # Ensure epsilon matches z_mean dtype\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim), dtype=z_mean.dtype)\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n\n # Encoder with dtype policy awareness\n # padding=same :ensure that the output feature maps have the same spatial dimensions (height and width, or height, width, and depth in 3D) as the input volume\ndef create_encoder(input_shape):\n     encoder_inputs = keras.Input(shape=input_shape)\n    x = layers.Conv3D(64, 3, activation='relu', padding='same')(encoder_inputs)\n    x = layers.MaxPooling3D((2, 2, 2))(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.2)(x)\n    \n    x = layers.Conv3D(128, 3, activation='relu', padding='same')(x)\n    x = layers.MaxPooling3D((2, 2, 2))(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n\n    x = layers.Conv3D(128, 3, activation='relu', padding='same')(x)\n    x = layers.MaxPooling3D((2, 2, 2))(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    \n    x = layers.Conv3D(256, 3, activation='relu', padding='same')(x)\n    x = layers.MaxPooling3D((2, 2, 2))(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.4)(x)\n\n    x = layers.GlobalAveragePooling3D()(x)\n    # resnet_model = Sequential()\n\n    # pretrained_model= tf.keras.applications.ResNet50(include_top=False,\n    #                    input_shape=encoder_inputs,\n    #                    pooling='avg',\n    #                    weights='imagenet')\n    # for layer in pretrained_model.layers:\n    #         layer.trainable=False\n    \n    #     # Apply the pretrained model to the inputs\n    # x = pretrained_model(inputs)\n    \n    # # Flatten the output\n    # x = layers.Flatten()(x)\n    # x = layers.Dense()\n    \n    \n    \n    # Ensure output layers match policy\n    z_mean = layers.Dense(latent_dim, name='z_mean', dtype='float32')(x)\n    z_log_var = layers.Dense(latent_dim, name='z_log_var', dtype='float32')(x)\n    z = Sampling()([z_mean, z_log_var])\n    \n    return keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n\n# Decoder with dtype policy awareness\ndef create_decoder(output_shape):\n    latent_inputs = keras.Input(shape=(latent_dim,), dtype='float32')\n    \n    x = layers.Dense(15*15*6*256, activation='relu')(latent_inputs)\n    x = layers.Reshape((15,15,6,256))(x)\n    x = layers.Dropout(0.4)(x)\n    \n    x = layers.Conv3DTranspose(256, 3, strides=2, activation='relu', padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    \n    x = layers.Conv3DTranspose(128, 3, strides=2, activation='relu', padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n\n    x = layers.Conv3DTranspose(128, 3, strides=2, activation='relu', padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.2)(x)\n    \n    x = layers.Conv3DTranspose(64, 3, strides=2, activation='relu', padding='same')(x)\n    x = layers.BatchNormalization()(x)\n\n    decoder_outputs = layers.Conv3D(output_shape[-1], 3, activation='sigmoid', padding='same', dtype='float32')(x)\n    \n    # Dynamic cropping with dtype awareness\n    def crop_to_match(x):\n        input_shape = tf.shape(x)\n        crop_h = (input_shape[1] - output_shape[0]) // 2\n        crop_w = (input_shape[2] - output_shape[1]) // 2\n        crop_d = (input_shape[3] - output_shape[2]) // 2\n        \n        return x[:, \n               tf.maximum(crop_h, 0):tf.maximum(crop_h, 0)+output_shape[0],\n               tf.maximum(crop_w, 0):tf.maximum(crop_w, 0)+output_shape[1],\n               tf.maximum(crop_d, 0):tf.maximum(crop_d, 0)+output_shape[2],\n               :]\n    \n    decoder_outputs = layers.Lambda(crop_to_match, dtype='float32')(decoder_outputs)\n    \n    return keras.Model(latent_inputs, decoder_outputs, name='decoder')\n\n# CVAE model with mixed precision handling\nclass CVAE(keras.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super().__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n        \n    def call(self, inputs):\n        z_mean, z_log_var, z = self.encoder(inputs)\n        reconstruction = self.decoder(z)\n        return reconstruction\n\n    \n    def train_step(self, data):\n        if isinstance(data, tuple):\n            data = data[0]\n            \n        with tf.GradientTape() as tape:\n            z_mean, z_log_var, z = self.encoder(data)\n            reconstruction = self.decoder(z)\n            \n            # Cast to float32 for loss calculation if using mixed precision\n            data_f32 = tf.cast(data, tf.float32)\n            reconstruction_f32 = tf.cast(reconstruction, tf.float32)\n            \n            reconstruction_loss = tf.reduce_mean(\n                keras.losses.mean_squared_error(data_f32, reconstruction_f32)\n            )\n            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n            total_loss = reconstruction_loss + kl_loss\n            \n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        \n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.kl_loss_tracker.update_state(kl_loss)\n        \n        return {m.name: m.result() for m in self.metrics}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:09:18.017992Z","iopub.execute_input":"2025-04-21T10:09:18.018329Z","iopub.status.idle":"2025-04-21T10:09:18.040791Z","shell.execute_reply.started":"2025-04-21T10:09:18.018301Z","shell.execute_reply":"2025-04-21T10:09:18.039914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_cvae.build((None, *INPUT_SHAPE))\n# Now summary will work\nprint(\"=\"*80)\nprint(\"ENCODER SUMMARY:\")\nencoder.summary()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"DECODER SUMMARY:\")\ndecoder.summary()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"FULL VAE SUMMARY:\")\ncvae.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T11:12:20.990189Z","iopub.execute_input":"2025-04-21T11:12:20.990466Z","iopub.status.idle":"2025-04-21T11:12:21.171247Z","shell.execute_reply.started":"2025-04-21T11:12:20.990443Z","shell.execute_reply":"2025-04-21T11:12:21.170190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Training data loading\ndef load_brats_data(path, patient_id):\n    modalities = []\n    for mod in ['flair', 't1', 't1ce', 't2']:\n        vol = nib.load(f\"{path}/BraTS20_Training_{patient_id:03d}/BraTS20_Training_{patient_id:03d}_{mod}.nii\").get_fdata()\n        vol = (vol - vol.min()) / (vol.max() - vol.min())\n        modalities.append(vol[..., VOLUME_START_AT:VOLUME_START_AT+VOLUME_SLICES])\n    return np.stack(modalities, axis=-1)\n\n\ndef train_cvae(train_data):\n    # adding the early stoping and dropout for better performance (date 19-04-25)\n\n    if len(train_data) == 0:\n        raise ValueError(\"No training data could be loaded. Please check the dataset path.\")\n    \n    # Create data generators with batching\n    train_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(batch_size).shuffle(buffer_size=100).prefetch(tf.data.AUTOTUNE)\n    test_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    \n    # Create and compile model\n    encoder = create_encoder(INPUT_SHAPE)\n    decoder = create_decoder(INPUT_SHAPE)\n    cvae = CVAE(encoder, decoder)\n    \n    # Configure mixed precision\n    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n    tf.keras.mixed_precision.set_global_policy(policy)\n    \n    # Optimizer with learning rate schedule\n    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=1e-4,\n        decay_steps=10000,\n        decay_rate=0.9)\n    \n    optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n    cvae.compile(optimizer=optimizer)\n    \n    # Callbacks\n    early_stopping = keras.callbacks.EarlyStopping(\n        monitor='val_total_loss',\n        patience=15,\n        restore_best_weights=True,\n        min_delta=0.001)\n    \n    checkpoint = keras.callbacks.ModelCheckpoint(\n        'best_cvae_model.h5',\n        monitor='val_total_loss',\n        save_best_only=True)\n\n    # Training loop\n    history = cvae.fit(\n        train_data,\n        epochs=60,\n        batch_size=batch_size,\n        shuffle=True\n    )\n\n    return cvae, history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:09:25.320428Z","iopub.execute_input":"2025-04-21T10:09:25.320719Z","iopub.status.idle":"2025-04-21T10:09:25.329262Z","shell.execute_reply.started":"2025-04-21T10:09:25.320694Z","shell.execute_reply":"2025-04-21T10:09:25.328324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = np.array([load_brats_data(TRAIN_DATASET_PATH ,i)for i in range(1,31)])\nlen(train_data)\n# Train-test split\ntrain_data, test_data = train_test_split(train_data, test_size=0.2, random_state=42)\nlen(train_data),len(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:09:30.674725Z","iopub.execute_input":"2025-04-21T10:09:30.675006Z","iopub.status.idle":"2025-04-21T10:10:04.202852Z","shell.execute_reply.started":"2025-04-21T10:09:30.674983Z","shell.execute_reply":"2025-04-21T10:10:04.202164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_cvae, training_history = train_cvae(train_data)\n# Save the final model\ntrained_cvae.save_weights('cvae_final_weights.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:10:34.943603Z","iopub.execute_input":"2025-04-21T10:10:34.943902Z","iopub.status.idle":"2025-04-21T11:07:36.560230Z","shell.execute_reply.started":"2025-04-21T10:10:34.943878Z","shell.execute_reply":"2025-04-21T11:07:36.559285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport nibabel as nib\nimport numpy as np\n\ndef generate_and_save_images(model, input_data, output_path, num_samples=3):\n    \"\"\"\n    Generate reconstructed images from input data and save them as NIfTI files\n    and 2D slices as PNG images.\n    \n    Args:\n        model: Trained CVAE model\n        input_data: Input MRI volumes (shape: [num_samples, 240, 240, 48, 4])\n        output_path: Directory to save outputs\n        num_samples: Number of samples to generate and visualize\n    \"\"\"\n    # Ensure output directory exists\n    os.makedirs(output_path, exist_ok=True)\n    \n    # Get reconstructions\n    _, _, z = model.encoder.predict(input_data[:num_samples])\n    reconstructions = model.decoder.predict(z)\n    \n    # Convert to numpy array and ensure float32\n    reconstructions = np.array(reconstructions, dtype=np.float32)\n    \n    # Save each modality separately\n    modalities = ['flair', 't1', 't1ce', 't2']\n    \n    for i in range(num_samples):\n        # Save original and reconstructed as NIfTI\n        for mod_idx, mod in enumerate(modalities):\n            # Original\n            orig_vol = nib.Nifti1Image(input_data[i,...,mod_idx], np.eye(4))\n            nib.save(orig_vol, f\"{output_path}/sample_{i}_original_{mod}.nii.gz\")\n            \n            # Reconstructed\n            recon_vol = nib.Nifti1Image(reconstructions[i,...,mod_idx], np.eye(4))\n            nib.save(recon_vol, f\"{output_path}/sample_{i}_reconstructed_{mod}.nii.gz\")\n        \n        # Visualize slices\n        visualize_slices(input_data[i], reconstructions[i], \n                         save_path=f\"{output_path}/sample_{i}_comparison.png\")\n\ndef visualize_slices(original, reconstructed, save_path=None, num_slices=3):\n    \"\"\"\n    Visualize comparison between original and reconstructed slices.\n    \n    Args:\n        original: Original volume (240, 240, 48, 4)\n        reconstructed: Reconstructed volume (240, 240, 48, 4)\n        save_path: Path to save the figure (if None, shows interactively)\n        num_slices: Number of slices to display\n    \"\"\"\n    modalities = ['FLAIR', 'T1', 'T1ce', 'T2']\n    slice_indices = np.linspace(0, original.shape[2]-1, num_slices, dtype=int)\n    \n    plt.figure(figsize=(20, 6*num_slices))\n    \n    for i, slice_idx in enumerate(slice_indices):\n        for mod_idx, mod in enumerate(modalities):\n            # Original\n            plt.subplot(num_slices, 8, i*8 + mod_idx*2 + 1)\n            plt.imshow(original[..., slice_idx, mod_idx], cmap='gray')\n            plt.title(f\"Original {mod}\\nSlice {slice_idx}\")\n            plt.axis('off')\n            \n            # Reconstructed\n            plt.subplot(num_slices, 8, i*8 + mod_idx*2 + 2)\n            plt.imshow(reconstructed[..., slice_idx, mod_idx], cmap='gray')\n            plt.title(f\"Reconstructed {mod}\")\n            plt.axis('off')\n    \n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path)\n        plt.close()\n    else:\n        plt.show()\n\ndef visualize_latent_space(model, data, save_path=None):\n    \"\"\"\n    Visualize the latent space distribution using PCA or t-SNE.\n    \n    Args:\n        model: Trained CVAE model\n        data: Input data to encode\n        save_path: Path to save the figure\n    \"\"\"\n    from sklearn.decomposition import PCA\n    from sklearn.manifold import TSNE\n    \n    # Get latent representations\n    z_mean, _, _ = model.encoder.predict(data)\n    \n    # Reduce dimensionality\n    pca = PCA(n_components=2)\n    z_pca = pca.fit_transform(z_mean)\n    \n    tsne = TSNE(n_components=2, perplexity=min(30, z_mean.shape[0]-1))\n    z_tsne = tsne.fit_transform(z_mean)\n    \n    # Plot\n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    plt.scatter(z_pca[:, 0], z_pca[:, 1])\n    plt.title(\"PCA of Latent Space\")\n    plt.xlabel(\"PC1\")\n    plt.ylabel(\"PC2\")\n    \n    plt.subplot(1, 2, 2)\n    plt.scatter(z_tsne[:, 0], z_tsne[:, 1])\n    plt.title(\"t-SNE of Latent Space\")\n    plt.xlabel(\"Dimension 1\")\n    plt.ylabel(\"Dimension 2\")\n    \n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path)\n        plt.close()\n    else:\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T11:13:00.006728Z","iopub.execute_input":"2025-04-21T11:13:00.007034Z","iopub.status.idle":"2025-04-21T11:13:00.024356Z","shell.execute_reply.started":"2025-04-21T11:13:00.006996Z","shell.execute_reply":"2025-04-21T11:13:00.023626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport nibabel as nib\nimport numpy as np\nfrom tqdm import tqdm\n\n# Verify paths (Kaggle specific)\nVALIDATION_DATASET_PATH = \"/kaggle/input/brats20-dataset-training-validation/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData\"\noutput_dir = \"/kaggle/working/reconstruction_results\"\n\n# Create output directory\nos.makedirs(output_dir, exist_ok=True)\n\n# Find available validation patients\ndef find_validation_patients(base_path, start=1, end=10):\n    valid_patients = []\n    for i in range(start, end+1):\n        path_3d = f\"{base_path}/BraTS20_Validation_{i:03d}\"\n        path_flat = f\"{base_path}/BraTS20_Validation_{i:03d}_flair.nii.gz\"\n        if os.path.exists(path_3d) or os.path.exists(path_flat):\n            valid_patients.append(i)\n    return valid_patients\n\navailable_patients = find_validation_patients(VALIDATION_DATASET_PATH)\nprint(f\"Found {len(available_patients)} validation patients: {available_patients}\")\n\n# Load first 3 available patients\n\n# test_data = []\n# for i in available_patients[:3]:\n#     try:\n#         data = load_brats_Validationdata(VALIDATION_DATASET_PATH, i)\n#         test_data.append(data)\n#         print(f\"Successfully loaded patient {i}\")\n#     except Exception as e:\n#         print(f\"Error loading patient {i}: {str(e)}\")\n\n# if len(test_data) > 0:\n#     test_data = np.array(train_data)\n#     print(f\"Loaded test data shape: {train_data.shape}\")\n    \n# Generate and save reconstructions\ngenerate_and_save_images(trained_cvae, test_data, output_dir, num_samples=min(3, len(train_data)))\n\n# Visualize latent space\nvisualize_latent_space(trained_cvae, test_data, save_path=f\"{output_dir}/latent_space.png\")\n\n\n#     # Verify output files were created\n#     print(\"\\nGenerated files:\")\n#     !ls -lh {output_dir}\n# else:\n#     print(\"No validation data could be loaded. Please check the dataset path.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T11:13:36.308292Z","iopub.execute_input":"2025-04-21T11:13:36.308575Z","iopub.status.idle":"2025-04-21T11:13:47.334722Z","shell.execute_reply.started":"2025-04-21T11:13:36.308553Z","shell.execute_reply":"2025-04-21T11:13:47.333256Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Today is 21 april 2025 ,i am working on this project but the memory issues are there i need to spend time in order to resolve the issue,i will resume working on this on 1 may 2025 <br>\n## The first task would be to use the resnet50 network for the encoder and decoder,since the model is traine on 2D data and i am working on 3D data i ahve to maek some changes in the architechture.\n## the second task would be to get rid of the memory issue while validating the model.\n\nsee the deepseek REplacing Encoder and Decoder with resnet50 for \nreferences[https://chat.deepseek.com/a/chat/s/6f76c9c7-a084-4a48-a4c7-2aba94783381](http://)\n\nreference2:[https://github.com/nachi-hebbar/Transfer-Learning-ResNet-Keras/blob/main/ResNet_50.ipynb](http://)","metadata":{}},{"cell_type":"code","source":"def display_from_memory(test_data, reconstructions, sample_idx=0, slice_idx=24):\n    \"\"\"Display comparison using in-memory arrays\"\"\"\n    modalities = ['FLAIR', 'T1', 'T1ce', 'T2']\n    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n    \n    for mod_idx, mod in enumerate(modalities):\n        # Original\n        axes[0, mod_idx].imshow(test_data[sample_idx, :, :, slice_idx, mod_idx], cmap='gray')\n        axes[0, mod_idx].set_title(f'Original {mod}')\n        axes[0, mod_idx].axis('off')\n        \n        # Reconstructed\n        axes[1, mod_idx].imshow(reconstructions[sample_idx, :, :, slice_idx, mod_idx], cmap='gray')\n        axes[1, mod_idx].set_title(f'Reconstructed {mod}')\n        axes[1, mod_idx].axis('off')\n    \n    plt.suptitle(f'Patient {sample_idx+1} - Slice {slice_idx} Comparison', y=1.02, fontsize=16)\n    plt.tight_layout()\n    plt.show()\n\n# To use this version:\n# 1. First get reconstructions for your test_data\n_, _, z = cvae_trained.encoder.predict(train_data)\nreconstructions = cvae_trained.decoder.predict(z)\n\n# 2. Then display\ndisplay_from_memory(train_data, reconstructions, sample_idx=0, slice_idx=24)\n\n# Interactive version\ninteract(lambda sample, slice: display_from_memory(train_data, reconstructions, sample, slice),\n         sample=IntSlider(min=0, max=len(train_data)-1, value=0),\n         slice=IntSlider(min=0, max=47, value=24));","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T11:20:21.260353Z","iopub.execute_input":"2025-04-21T11:20:21.260668Z","iopub.status.idle":"2025-04-21T11:20:21.284252Z","shell.execute_reply.started":"2025-04-21T11:20:21.260637Z","shell.execute_reply":"2025-04-21T11:20:21.282948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Image\n\ndef show_saved_comparison(sample_idx):\n    img_path = f\"/kaggle/working/reconstruction_results/sample_{sample_idx}_comparison.png\"\n    return Image(filename=img_path)\n\n# Display all three comparisons\nfor i in range(3):\n    display(show_saved_comparison(i))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T05:19:50.304382Z","iopub.status.idle":"2025-04-19T05:19:50.304941Z","shell.execute_reply":"2025-04-19T05:19:50.304649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming you have your trained cvae model and test data\n#for validation dataset\ndef load_brats_Validationdata(path, patient_id):\n    modalities = []\n    for mod in ['flair', 't1', 't1ce', 't2']:\n        vol = nib.load(f\"{path}/BraTS20_Validation_{patient_id:03d}/BraTS20_Validation_{patient_id:03d}_{mod}.nii\").get_fdata()\n        vol = (vol - vol.min()) / (vol.max() - vol.min())\n        modalities.append(vol[..., VOLUME_START_AT:VOLUME_START_AT+VOLUME_SLICES])\n    return np.stack(modalities, axis=-1)\n    \noutput_dir = \"/kaggle/working/reconstruction_results\"\n\n# Load test data (similar to how you loaded training data)\ntest_data = np.array([load_brats_Validationdata(VALIDATION_DATASET_PATH, i) for i in range(4, 7)])\n\n# Generate and save reconstructions\ngenerate_and_save_images(cvae, test_data, output_dir, num_samples=3)\n\n# Visualize latent space\nvisualize_latent_space(cvae, test_data, save_path=f\"{output_dir}/latent_space.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T05:19:50.306050Z","iopub.status.idle":"2025-04-19T05:19:50.306586Z","shell.execute_reply":"2025-04-19T05:19:50.306304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import tensorflow as tf\n# from tensorflow import keras\n# from tensorflow.keras import layers\n# from sklearn.model_selection import train_test_split\n# import nibabel as nib\n# import os\n# from tqdm import tqdm\n\n# # Parameters\n# VOLUME_SLICES = 96\n# VOLUME_START_AT = 22\n# INPUT_SHAPE = (240, 240, VOLUME_SLICES, 4)\n# latent_dim = 256\n# batch_size = 4  # Increased batch size for better GPU utilization\n# epochs = 100\n\n# # Sampling layer with explicit dtype handling\n# class Sampling(layers.Layer):\n#     def call(self, inputs):\n#         z_mean, z_log_var = inputs\n#         batch = tf.shape(z_mean)[0]\n#         dim = tf.shape(z_mean)[1]\n        \n#         epsilon = tf.keras.backend.random_normal(shape=(batch, dim), dtype=z_mean.dtype)\n#         return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n\n# # Enhanced Encoder with dropout\n# def create_encoder(input_shape):\n#     encoder_inputs = keras.Input(shape=input_shape)\n    \n#     x = layers.Conv3D(64, 3, activation='relu', padding='same')(encoder_inputs)\n#     x = layers.MaxPooling3D((2, 2, 2))(x)\n#     x = layers.BatchNormalization()(x)\n#     x = layers.Dropout(0.2)(x)\n    \n#     x = layers.Conv3D(128, 3, activation='relu', padding='same')(x)\n#     x = layers.MaxPooling3D((2, 2, 2))(x)\n#     x = layers.BatchNormalization()(x)\n#     x = layers.Dropout(0.3)(x)\n\n#     x = layers.Conv3D(128, 3, activation='relu', padding='same')(x)\n#     x = layers.MaxPooling3D((2, 2, 2))(x)\n#     x = layers.BatchNormalization()(x)\n#     x = layers.Dropout(0.3)(x)\n    \n#     x = layers.Conv3D(256, 3, activation='relu', padding='same')(x)\n#     x = layers.MaxPooling3D((2, 2, 2))(x)\n#     x = layers.BatchNormalization()(x)\n#     x = layers.Dropout(0.4)(x)\n    \n#     x = layers.GlobalAveragePooling3D()(x)\n    \n#     z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n#     z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n#     z = Sampling()([z_mean, z_log_var])\n    \n#     return keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n\n# # Enhanced Decoder with dropout\n# def create_decoder(output_shape):\n#     latent_inputs = keras.Input(shape=(latent_dim,))\n    \n#     x = layers.Dense(15*15*6*256, activation='relu')(latent_inputs)\n#     x = layers.Reshape((15,15,6,256))(x)\n#     x = layers.Dropout(0.4)(x)\n    \n#     x = layers.Conv3DTranspose(256, 3, strides=2, activation='relu', padding='same')(x)\n#     x = layers.BatchNormalization()(x)\n#     x = layers.Dropout(0.3)(x)\n    \n#     x = layers.Conv3DTranspose(128, 3, strides=2, activation='relu', padding='same')(x)\n#     x = layers.BatchNormalization()(x)\n#     x = layers.Dropout(0.3)(x)\n\n#     x = layers.Conv3DTranspose(128, 3, strides=2, activation='relu', padding='same')(x)\n#     x = layers.BatchNormalization()(x)\n#     x = layers.Dropout(0.2)(x)\n    \n#     x = layers.Conv3DTranspose(64, 3, strides=2, activation='relu', padding='same')(x)\n#     x = layers.BatchNormalization()(x)\n\n#     decoder_outputs = layers.Conv3D(output_shape[-1], 3, activation='sigmoid', padding='same')(x)\n    \n#     def crop_to_match(x):\n#         input_shape = tf.shape(x)\n#         crop_h = (input_shape[1] - output_shape[0]) // 2\n#         crop_w = (input_shape[2] - output_shape[1]) // 2\n#         crop_d = (input_shape[3] - output_shape[2]) // 2\n        \n#         return x[:, \n#                tf.maximum(crop_h, 0):tf.maximum(crop_h, 0)+output_shape[0],\n#                tf.maximum(crop_w, 0):tf.maximum(crop_w, 0)+output_shape[1],\n#                tf.maximum(crop_d, 0):tf.maximum(crop_d, 0)+output_shape[2],\n#                :]\n    \n#     decoder_outputs = layers.Lambda(crop_to_match)(decoder_outputs)\n    \n#     return keras.Model(latent_inputs, decoder_outputs, name='decoder')\n\n# # CVAE model\n# class CVAE(keras.Model):\n#     def __init__(self, encoder, decoder, **kwargs):\n#         super().__init__(**kwargs)\n#         self.encoder = encoder\n#         self.decoder = decoder\n#         self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n#         self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n#         self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n        \n#     def call(self, inputs):\n#         z_mean, z_log_var, z = self.encoder(inputs)\n#         reconstruction = self.decoder(z)\n#         return reconstruction\n    \n#     def train_step(self, data):\n#         if isinstance(data, tuple):\n#             data = data[0]\n            \n#         with tf.GradientTape() as tape:\n#             z_mean, z_log_var, z = self.encoder(data)\n#             reconstruction = self.decoder(z)\n            \n#             reconstruction_loss = tf.reduce_mean(\n#                 keras.losses.mean_squared_error(data, reconstruction)\n#             )\n#             kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n#             kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n#             total_loss = reconstruction_loss + kl_loss\n            \n#         grads = tape.gradient(total_loss, self.trainable_weights)\n#         self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        \n#         self.total_loss_tracker.update_state(total_loss)\n#         self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n#         self.kl_loss_tracker.update_state(kl_loss)\n        \n#         return {m.name: m.result() for m in self.metrics}\n\n# # Data loading functions\n# def load_brats_data(path, patient_id):\n#     modalities = []\n#     for mod in ['flair', 't1', 't1ce', 't2']:\n#         try:\n#             # Try both possible file naming conventions\n#             try:\n#                 vol = nib.load(f\"{path}/BraTS20_Training_{patient_id:03d}/BraTS20_Training_{patient_id:03d}_{mod}.nii\").get_fdata()\n#             except:\n#                 vol = nib.load(f\"{path}/BraTS20_Training_{patient_id:03d}_{mod}.nii.gz\").get_fdata()\n            \n#             # Normalize and select slices\n#             vol = (vol - vol.min()) / (vol.max() - vol.min())\n#             modalities.append(vol[..., VOLUME_START_AT:VOLUME_START_AT+VOLUME_SLICES])\n#         except Exception as e:\n#             print(f\"Error loading patient {patient_id} modality {mod}: {str(e)}\")\n#             return None\n    \n#     return np.stack(modalities, axis=-1)\n\n# def load_dataset(path, num_patients=None):\n#     patient_ids = []\n#     for i in range(1, 1000):  # Check up to 1000 patients\n#         if os.path.exists(f\"{path}/BraTS20_Training_{i:03d}\") or \\\n#            os.path.exists(f\"{path}/BraTS20_Training_{i:03d}_flair.nii.gz\"):\n#             patient_ids.append(i)\n#             if num_patients and len(patient_ids) >= num_patients:\n#                 break\n    \n#     data = []\n#     for pid in tqdm(patient_ids, desc=\"Loading data\"):\n#         patient_data = load_brats_data(path, pid)\n#         if patient_data is not None:\n#             data.append(patient_data)\n    \n#     return np.array(data)\n\n# # Main training function\n# def train_cvae():\n#     # Load and preprocess data\n#     train_data = load_dataset(\"/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\", num_patients=100)\n    \n#     if len(train_data) == 0:\n#         raise ValueError(\"No training data could be loaded. Please check the dataset path.\")\n    \n#     # Train-test split\n#     train_data, test_data = train_test_split(train_data, test_size=0.2, random_state=42)\n    \n#     # Create data generators with batching\n#     train_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(batch_size).shuffle(buffer_size=100).prefetch(tf.data.AUTOTUNE)\n#     test_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    \n#     # Create and compile model\n#     encoder = create_encoder(INPUT_SHAPE)\n#     decoder = create_decoder(INPUT_SHAPE)\n#     cvae = CVAE(encoder, decoder)\n    \n#     # Configure mixed precision\n#     policy = tf.keras.mixed_precision.Policy('mixed_float16')\n#     tf.keras.mixed_precision.set_global_policy(policy)\n    \n#     # Optimizer with learning rate schedule\n#     lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n#         initial_learning_rate=1e-4,\n#         decay_steps=10000,\n#         decay_rate=0.9)\n    \n#     optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n#     cvae.compile(optimizer=optimizer)\n    \n#     # Callbacks\n#     early_stopping = keras.callbacks.EarlyStopping(\n#         monitor='val_total_loss',\n#         patience=15,\n#         restore_best_weights=True,\n#         min_delta=0.001)\n    \n#     checkpoint = keras.callbacks.ModelCheckpoint(\n#         'best_cvae_model.h5',\n#         monitor='val_total_loss',\n#         save_best_only=True)\n    \n#     # Train the model\n#     history = cvae.fit(\n#         train_dataset,\n#         validation_data=test_dataset,\n#         epochs=epochs,\n#         callbacks=[early_stopping, checkpoint]\n#     )\n    \n#     return cvae, history\n\n# # Run training\n# if __name__ == \"__main__\":\n#     trained_cvae, training_history = train_cvae(\n    \n#     # Save the final model\n#     trained_cvae.save_weights('cvae_final_weights.h5')\n    \n#     # Visualization code would go here\n#     # generate_and_save_images(trained_cvae, ...)\n#     # visualize_latent_space(trained_cvae, ...)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T05:22:19.322900Z","iopub.execute_input":"2025-04-19T05:22:19.323270Z","iopub.status.idle":"2025-04-19T05:22:19.330138Z","shell.execute_reply.started":"2025-04-19T05:22:19.323186Z","shell.execute_reply":"2025-04-19T05:22:19.329112Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MRI segmentation","metadata":{}},{"cell_type":"code","source":"# DEFINE seg-areas  \nSEGMENT_CLASSES = {\n    0 : 'NOT tumor',\n    1 : 'NECROTIC/CORE', # or NON-ENHANCING tumor CORE\n    2 : 'EDEMA',\n    3 : 'ENHANCING' # original 4 -> converted into 3 later\n}\n\n\n# there are 155 slices per volume\n# to start at 5 and use 145 slices means we will skip the first 5 and last 5 \nVOLUME_SLICES = 100 \nVOLUME_START_AT = 22 # first slice of volume that we will include","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T05:31:23.559883Z","iopub.execute_input":"2025-04-14T05:31:23.560179Z","iopub.status.idle":"2025-04-14T05:31:23.564295Z","shell.execute_reply.started":"2025-04-14T05:31:23.560155Z","shell.execute_reply":"2025-04-14T05:31:23.563444Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Image data descriptions\n\nAll BraTS multimodal scans are available as  NIfTI files (.nii.gz) -> commonly used medical imaging format to store brain imagin data obtained using MRI and describe different MRI settings \n1. **T1**: T1-weighted, native image, sagittal or axial 2D acquisitions, with 1–6 mm slice thickness.\n2. **T1c**: T1-weighted, contrast-enhanced (Gadolinium) image, with 3D acquisition and 1 mm isotropic voxel size for most patients.\n3. **T2**: T2-weighted image, axial 2D acquisition, with 2–6 mm slice thickness.\n4. **FLAIR**: T2-weighted FLAIR image, axial, coronal, or sagittal 2D acquisitions, 2–6 mm slice thickness.\n\nData were acquired with different clinical protocols and various scanners from multiple (n=19) institutions.\n\nAll the imaging datasets have been segmented manually, by one to four raters, following the same annotation protocol, and their annotations were approved by experienced neuro-radiologists. Annotations comprise the GD-enhancing tumor (ET — label 4), the peritumoral edema (ED — label 2), and the necrotic and non-enhancing tumor core (NCR/NET — label 1), as described both in the BraTS 2012-2013 TMI paper and in the latest BraTS summarizing paper. The provided data are distributed after their pre-processing, i.e., co-registered to the same anatomical template, interpolated to the same resolution (1 mm^3) and skull-stripped.\n\n","metadata":{}},{"cell_type":"code","source":"TRAIN_DATASET_PATH = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/'\nVALIDATION_DATASET_PATH = '../input/brats20-dataset-training-validation/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData'\n\ntest_image_flair=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_flair.nii').get_fdata()\ntest_image_t1=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_t1.nii').get_fdata()\ntest_image_t1ce=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_t1ce.nii').get_fdata()\ntest_image_t2=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_t2.nii').get_fdata()\ntest_mask=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_seg.nii').get_fdata()\n\n\nfig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5, figsize = (20, 10))\nslice_w = 25\nax1.imshow(test_image_flair[:,:,test_image_flair.shape[0]//2-slice_w], cmap = 'gray')\nax1.set_title('Image flair')\nax2.imshow(test_image_t1[:,:,test_image_t1.shape[0]//2-slice_w], cmap = 'gray')\nax2.set_title('Image t1')\nax3.imshow(test_image_t1ce[:,:,test_image_t1ce.shape[0]//2-slice_w], cmap = 'gray')\nax3.set_title('Image t1ce')\nax4.imshow(test_image_t2[:,:,test_image_t2.shape[0]//2-slice_w], cmap = 'gray')\nax4.set_title('Image t2')\nax5.imshow(test_mask[:,:,test_mask.shape[0]//2-slice_w])\nax5.set_title('Mask')\n","metadata":{"_kg_hide-input":true,"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T05:53:15.369545Z","iopub.execute_input":"2025-04-14T05:53:15.369869Z","iopub.status.idle":"2025-04-14T05:53:16.125506Z","shell.execute_reply.started":"2025-04-14T05:53:15.369842Z","shell.execute_reply":"2025-04-14T05:53:16.124783Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Show whole nifti data -> print each slice from 3d data**","metadata":{}},{"cell_type":"code","source":"# Skip 50:-50 slices since there is not much to see\nfig, ax1 = plt.subplots(1, 1, figsize = (15,15))\nax1.imshow(rotate(montage(test_image_t1[50:-50,:,:]), 90, resize=True), cmap ='gray')","metadata":{"_kg_hide-input":true,"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:51:39.808838Z","iopub.execute_input":"2025-04-13T08:51:39.809082Z","iopub.status.idle":"2025-04-13T08:51:40.716412Z","shell.execute_reply.started":"2025-04-13T08:51:39.809057Z","shell.execute_reply":"2025-04-13T08:51:40.715724Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Show segment of tumor for each above slice**","metadata":{}},{"cell_type":"code","source":"# Skip 50:-50 slices since there is not much to see\nfig, ax1 = plt.subplots(1, 1, figsize = (15,15))\nax1.imshow(rotate(montage(test_mask[60:-60,:,:]), 90, resize=True), cmap ='gray')","metadata":{"_kg_hide-input":true,"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:51:40.718706Z","iopub.execute_input":"2025-04-13T08:51:40.718982Z","iopub.status.idle":"2025-04-13T08:51:41.447282Z","shell.execute_reply.started":"2025-04-13T08:51:40.718956Z","shell.execute_reply":"2025-04-13T08:51:41.446452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shutil.copy2(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_flair.nii', './test_gif_BraTS20_Training_001_flair.nii')\ngif2nif.write_gif_normal('./test_gif_BraTS20_Training_001_flair.nii')","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:51:41.449408Z","iopub.execute_input":"2025-04-13T08:51:41.449711Z","iopub.status.idle":"2025-04-13T08:51:44.262555Z","shell.execute_reply.started":"2025-04-13T08:51:41.449679Z","shell.execute_reply":"2025-04-13T08:51:44.261904Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Gif representation of slices in 3D volume**\n<img src=\"https://media1.tenor.com/images/15427ffc1399afc3334f12fd27549a95/tenor.gif?itemid=20554734\">","metadata":{}},{"cell_type":"markdown","source":"**Show segments of tumor using different effects**","metadata":{}},{"cell_type":"code","source":"niimg = nl.image.load_img(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_flair.nii')\nnimask = nl.image.load_img(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_seg.nii')\n\nfig, axes = plt.subplots(nrows=4, figsize=(30, 40))\n\n\nnlplt.plot_anat(niimg,\n                title='BraTS20_Training_001_flair.nii plot_anat',\n                axes=axes[0])\n\nnlplt.plot_epi(niimg,\n               title='BraTS20_Training_001_flair.nii plot_epi',\n               axes=axes[1])\n\nnlplt.plot_img(niimg,\n               title='BraTS20_Training_001_flair.nii plot_img',\n               axes=axes[2])\n\nnlplt.plot_roi(nimask, \n               title='BraTS20_Training_001_flair.nii with mask plot_roi',\n               bg_img=niimg, \n               axes=axes[3], cmap='Paired')\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:51:44.263619Z","iopub.execute_input":"2025-04-13T08:51:44.263865Z","iopub.status.idle":"2025-04-13T08:51:50.910409Z","shell.execute_reply.started":"2025-04-13T08:51:44.263841Z","shell.execute_reply":"2025-04-13T08:51:50.909694Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create model || U-Net: Convolutional Networks for Biomedical Image Segmentation\nhe u-net is convolutional network architecture for fast and precise segmentation of images. Up to now it has outperformed the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. It has won the Grand Challenge for Computer-Automated Detection of Caries in Bitewing Radiography at ISBI 2015, and it has won the Cell Tracking Challenge at ISBI 2015 on the two most challenging transmitted light microscopy categories (Phase contrast and DIC microscopy) by a large margin\n[more on](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)\n![official definiton](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)\n","metadata":{}},{"cell_type":"markdown","source":"# Loss function\n**Dice coefficient**\n, which is essentially a measure of overlap between two samples. This measure ranges from 0 to 1 where a Dice coefficient of 1 denotes perfect and complete overlap. The Dice coefficient was originally developed for binary data, and can be calculated as:\n\n![dice loss](https://wikimedia.org/api/rest_v1/media/math/render/svg/a80a97215e1afc0b222e604af1b2099dc9363d3b)\n\n**As matrices**\n![dice loss](https://www.jeremyjordan.me/content/images/2018/05/intersection-1.png)\n\n[Implementation, (images above) and explanation can be found here](https://www.jeremyjordan.me/semantic-segmentation/)","metadata":{}},{"cell_type":"code","source":"# dice loss as defined above for 4 classes\ndef dice_coef(y_true, y_pred, smooth=1.0):\n    class_num = 4\n    for i in range(class_num):\n        y_true_f = K.flatten(y_true[:,:,:,i])\n        y_pred_f = K.flatten(y_pred[:,:,:,i])\n        intersection = K.sum(y_true_f * y_pred_f)\n        loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n   #     K.print_tensor(loss, message='loss value for class {} : '.format(SEGMENT_CLASSES[i]))\n        if i == 0:\n            total_loss = loss\n        else:\n            total_loss = total_loss + loss\n    total_loss = total_loss / class_num\n#    K.print_tensor(total_loss, message=' total dice coef: ')\n    return total_loss\n\n\n \n# define per class evaluation of dice coef\n# inspired by https://github.com/keras-team/keras/issues/9395\ndef dice_coef_necrotic(y_true, y_pred, epsilon=1e-6):\n    intersection = K.sum(K.abs(y_true[:,:,:,1] * y_pred[:,:,:,1]))\n    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,1])) + K.sum(K.square(y_pred[:,:,:,1])) + epsilon)\n\ndef dice_coef_edema(y_true, y_pred, epsilon=1e-6):\n    intersection = K.sum(K.abs(y_true[:,:,:,2] * y_pred[:,:,:,2]))\n    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,2])) + K.sum(K.square(y_pred[:,:,:,2])) + epsilon)\n\ndef dice_coef_enhancing(y_true, y_pred, epsilon=1e-6):\n    intersection = K.sum(K.abs(y_true[:,:,:,3] * y_pred[:,:,:,3]))\n    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,3])) + K.sum(K.square(y_pred[:,:,:,3])) + epsilon)\n\n\n\n# Computing Precision \ndef precision(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n\n    \n# Computing Sensitivity      \ndef sensitivity(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    return true_positives / (possible_positives + K.epsilon())\n\n\n# Computing Specificity\ndef specificity(y_true, y_pred):\n    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n    return true_negatives / (possible_negatives + K.epsilon())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:51:50.911834Z","iopub.execute_input":"2025-04-13T08:51:50.912138Z","iopub.status.idle":"2025-04-13T08:51:50.928037Z","shell.execute_reply.started":"2025-04-13T08:51:50.912104Z","shell.execute_reply":"2025-04-13T08:51:50.927153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMG_SIZE=128","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:51:50.929366Z","iopub.execute_input":"2025-04-13T08:51:50.929608Z","iopub.status.idle":"2025-04-13T08:51:50.941006Z","shell.execute_reply.started":"2025-04-13T08:51:50.929585Z","shell.execute_reply":"2025-04-13T08:51:50.940268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# source https://naomi-fridman.medium.com/multi-class-image-segmentation-a5cc671e647a\n\ndef build_unet(inputs, ker_init, dropout):\n    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(inputs)\n    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv1)\n    \n    pool = MaxPooling2D(pool_size=(2, 2))(conv1)\n    conv = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool)\n    conv = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv)\n    \n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool1)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv2)\n    \n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv3)\n    \n    \n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool4)\n    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv5)\n    drop5 = Dropout(dropout)(conv5)\n\n    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(drop5))\n    merge7 = concatenate([conv3,up7], axis = 3)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge7)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv7)\n\n    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv7))\n    merge8 = concatenate([conv2,up8], axis = 3)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge8)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv8)\n\n    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv8))\n    merge9 = concatenate([conv,up9], axis = 3)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge9)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv9)\n    \n    up = Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv9))\n    merge = concatenate([conv1,up], axis = 3)\n    conv = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge)\n    conv = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv)\n    \n    conv10 = Conv2D(4, (1,1), activation = 'softmax')(conv)\n    \n    return Model(inputs = inputs, outputs = conv10)\n\ninput_layer = Input((IMG_SIZE, IMG_SIZE, 2))\n\nmodel = build_unet(input_layer, 'he_normal', 0.2)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics = ['accuracy',tf.keras.metrics.MeanIoU(num_classes=4), dice_coef, precision, sensitivity, specificity, dice_coef_necrotic, dice_coef_edema ,dice_coef_enhancing] )","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:51:50.942031Z","iopub.execute_input":"2025-04-13T08:51:50.942281Z","iopub.status.idle":"2025-04-13T08:51:53.429970Z","shell.execute_reply.started":"2025-04-13T08:51:50.942240Z","shell.execute_reply":"2025-04-13T08:51:53.429284Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**model architecture** <br>\nIf you are about to use U-NET, I suggest to check out this awesome library that I found later, after manual implementation of U-NET [keras-unet-collection](https://pypi.org/project/keras-unet-collection/), which also contains implementation of dice loss, tversky loss and many more!","metadata":{}},{"cell_type":"code","source":"plot_model(model, \n           show_shapes = True,\n           show_dtype=False,\n           show_layer_names = True, \n           rankdir = 'TB', \n           expand_nested = False, \n           dpi = 70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:51:53.430906Z","iopub.execute_input":"2025-04-13T08:51:53.431119Z","iopub.status.idle":"2025-04-13T08:51:54.086966Z","shell.execute_reply.started":"2025-04-13T08:51:53.431097Z","shell.execute_reply":"2025-04-13T08:51:54.086074Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load data\nLoading all data into memory is not a good idea since the data are too big to fit in.\nSo we will create dataGenerators - load data on the fly as explained [here](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly)","metadata":{}},{"cell_type":"code","source":"# lists of directories with studies\ntrain_and_val_directories = [f.path for f in os.scandir(TRAIN_DATASET_PATH) if f.is_dir()]\n\n# file BraTS20_Training_355 has ill formatted name for for seg.nii file\ntrain_and_val_directories.remove(TRAIN_DATASET_PATH+'BraTS20_Training_355')\n\n\ndef pathListIntoIds(dirList):\n    x = []\n    for i in range(0,len(dirList)):\n        x.append(dirList[i][dirList[i].rfind('/')+1:])\n    return x\n\ntrain_and_test_ids = pathListIntoIds(train_and_val_directories); \n\n    \ntrain_test_ids, val_ids = train_test_split(train_and_test_ids,test_size=0.2) \ntrain_ids, test_ids = train_test_split(train_test_ids,test_size=0.15) ","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:51:54.088895Z","iopub.execute_input":"2025-04-13T08:51:54.089226Z","iopub.status.idle":"2025-04-13T08:51:54.840383Z","shell.execute_reply.started":"2025-04-13T08:51:54.089182Z","shell.execute_reply":"2025-04-13T08:51:54.839498Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Override Keras sequence DataGenerator class**","metadata":{}},{"cell_type":"code","source":"class DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, dim=(IMG_SIZE,IMG_SIZE), batch_size = 1, n_channels = 2, shuffle=True):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        Batch_ids = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(Batch_ids)\n\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, Batch_ids):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))\n        y = np.zeros((self.batch_size*VOLUME_SLICES, 240, 240))\n        Y = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, 4))\n\n        \n        # Generate data\n        for c, i in enumerate(Batch_ids):\n            case_path = os.path.join(TRAIN_DATASET_PATH, i)\n\n            data_path = os.path.join(case_path, f'{i}_flair.nii');\n            flair = nib.load(data_path).get_fdata()    \n\n            data_path = os.path.join(case_path, f'{i}_t1ce.nii');\n            ce = nib.load(data_path).get_fdata()\n            \n            data_path = os.path.join(case_path, f'{i}_seg.nii');\n            seg = nib.load(data_path).get_fdata()\n        \n            for j in range(VOLUME_SLICES):\n                 X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n                 X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n\n                 y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT];\n                    \n        # Generate masks\n        y[y==4] = 3;\n        mask = tf.one_hot(y, 4);\n        Y = tf.image.resize(mask, (IMG_SIZE, IMG_SIZE));\n        return X/np.max(X), Y\n        \ntraining_generator = DataGenerator(train_ids)\nvalid_generator = DataGenerator(val_ids)\ntest_generator = DataGenerator(test_ids)","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:51:54.841554Z","iopub.execute_input":"2025-04-13T08:51:54.841784Z","iopub.status.idle":"2025-04-13T08:51:54.853784Z","shell.execute_reply.started":"2025-04-13T08:51:54.841762Z","shell.execute_reply":"2025-04-13T08:51:54.853107Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Number of data used**\nfor training / testing / validation","metadata":{}},{"cell_type":"code","source":"# show number of data for each dir \ndef showDataLayout():\n    plt.bar([\"Train\",\"Valid\",\"Test\"],\n    [len(train_ids), len(val_ids), len(test_ids)], align='center',color=[ 'green','red', 'blue'])\n    plt.legend()\n\n    plt.ylabel('Number of images')\n    plt.title('Data distribution')\n\n    plt.show()\n    \nshowDataLayout()","metadata":{"_kg_hide-input":true,"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:51:54.854879Z","iopub.execute_input":"2025-04-13T08:51:54.855116Z","iopub.status.idle":"2025-04-13T08:51:54.964631Z","shell.execute_reply.started":"2025-04-13T08:51:54.855093Z","shell.execute_reply":"2025-04-13T08:51:54.963789Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Add callback for training process**","metadata":{}},{"cell_type":"code","source":"csv_logger = CSVLogger('training.log', separator=',', append=False)\n\n\ncallbacks = [\n#     keras.callbacks.EarlyStopping(monitor='loss', min_delta=0,\n#                               patience=2, verbose=1, mode='auto'),\n      keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=2, min_lr=0.000001, verbose=1),\n#  keras.callbacks.ModelCheckpoint(filepath = 'model_.{epoch:02d}-{val_loss:.6f}.m5',\n#                             verbose=1, save_best_only=True, save_weights_only = True)\n        csv_logger\n    ]","metadata":{"_kg_hide-input":true,"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:51:54.965751Z","iopub.execute_input":"2025-04-13T08:51:54.965994Z","iopub.status.idle":"2025-04-13T08:51:54.970163Z","shell.execute_reply.started":"2025-04-13T08:51:54.965967Z","shell.execute_reply":"2025-04-13T08:51:54.969320Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train model\nMy best model was trained with 81% accuracy on mean IOU and 65.5% on Dice loss <br>\nI will load this pretrained model instead of training again","metadata":{}},{"cell_type":"code","source":"K.clear_session()\n\n# history =  model.fit(training_generator,\n#                     epochs=35,\n#                     steps_per_epoch=len(train_ids),\n#                     callbacks= callbacks,\n#                     validation_data = valid_generator\n#                     )  \n# model.save(\"model_x1_1.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:51:54.971167Z","iopub.execute_input":"2025-04-13T08:51:54.971379Z","iopub.status.idle":"2025-04-13T08:51:54.985295Z","shell.execute_reply.started":"2025-04-13T08:51:54.971358Z","shell.execute_reply":"2025-04-13T08:51:54.984663Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Visualize the training process**","metadata":{}},{"cell_type":"code","source":"############ load trained model ################\nmodel = keras.models.load_model('../input/modelperclasseval/model_per_class.h5', \n                                   custom_objects={ 'accuracy' : tf.keras.metrics.MeanIoU(num_classes=4),\n                                                   \"dice_coef\": dice_coef,\n                                                   \"precision\": precision,\n                                                   \"sensitivity\":sensitivity,\n                                                   \"specificity\":specificity,\n                                                   \"dice_coef_necrotic\": dice_coef_necrotic,\n                                                   \"dice_coef_edema\": dice_coef_edema,\n                                                   \"dice_coef_enhancing\": dice_coef_enhancing\n                                                  }, compile=False)\n\nhistory = pd.read_csv('../input/modelperclasseval/training_per_class.log', sep=',', engine='python')\n\nhist=history\n\n############### ########## ####### #######\n\n# hist=history.history\n\nacc=hist['accuracy']\nval_acc=hist['val_accuracy']\n\nepoch=range(len(acc))\n\nloss=hist['loss']\nval_loss=hist['val_loss']\n\ntrain_dice=hist['dice_coef']\nval_dice=hist['val_dice_coef']\n\nf,ax=plt.subplots(1,4,figsize=(16,8))\n\nax[0].plot(epoch,acc,'b',label='Training Accuracy')\nax[0].plot(epoch,val_acc,'r',label='Validation Accuracy')\nax[0].legend()\n\nax[1].plot(epoch,loss,'b',label='Training Loss')\nax[1].plot(epoch,val_loss,'r',label='Validation Loss')\nax[1].legend()\n\nax[2].plot(epoch,train_dice,'b',label='Training dice coef')\nax[2].plot(epoch,val_dice,'r',label='Validation dice coef')\nax[2].legend()\n\nax[3].plot(epoch,hist['mean_io_u'],'b',label='Training mean IOU')\nax[3].plot(epoch,hist['val_mean_io_u'],'r',label='Validation mean IOU')\nax[3].legend()\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:51:54.986422Z","iopub.execute_input":"2025-04-13T08:51:54.986678Z","iopub.status.idle":"2025-04-13T08:51:56.190704Z","shell.execute_reply.started":"2025-04-13T08:51:54.986655Z","shell.execute_reply":"2025-04-13T08:51:56.189959Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prediction examples ","metadata":{}},{"cell_type":"code","source":"# mri type must one of 1) flair 2) t1 3) t1ce 4) t2 ------- or even 5) seg\n# returns volume of specified study at `path`\ndef imageLoader(path):\n    image = nib.load(path).get_fdata()\n    X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))\n    for j in range(VOLUME_SLICES):\n        X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(image[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n        X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n\n        y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT];\n    return np.array(image)\n\n\n# load nifti file at `path`\n# and load each slice with mask from volume\n# choose the mri type & resize to `IMG_SIZE`\ndef loadDataFromDir(path, list_of_files, mriType, n_images):\n    scans = []\n    masks = []\n    for i in list_of_files[:n_images]:\n        fullPath = glob.glob( i + '/*'+ mriType +'*')[0]\n        currentScanVolume = imageLoader(fullPath)\n        currentMaskVolume = imageLoader( glob.glob( i + '/*seg*')[0] ) \n        # for each slice in 3D volume, find also it's mask\n        for j in range(0, currentScanVolume.shape[2]):\n            scan_img = cv2.resize(currentScanVolume[:,:,j], dsize=(IMG_SIZE,IMG_SIZE), interpolation=cv2.INTER_AREA).astype('uint8')\n            mask_img = cv2.resize(currentMaskVolume[:,:,j], dsize=(IMG_SIZE,IMG_SIZE), interpolation=cv2.INTER_AREA).astype('uint8')\n            scans.append(scan_img[..., np.newaxis])\n            masks.append(mask_img[..., np.newaxis])\n    return np.array(scans, dtype='float32'), np.array(masks, dtype='float32')\n        \n#brains_list_test, masks_list_test = loadDataFromDir(VALIDATION_DATASET_PATH, test_directories, \"flair\", 5)\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:51:56.191764Z","iopub.execute_input":"2025-04-13T08:51:56.192001Z","iopub.status.idle":"2025-04-13T08:51:56.203224Z","shell.execute_reply.started":"2025-04-13T08:51:56.191977Z","shell.execute_reply":"2025-04-13T08:51:56.202475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predictByPath(case_path,case):\n    files = next(os.walk(case_path))[2]\n    X = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 2))\n  #  y = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE))\n    \n    vol_path = os.path.join(case_path, f'BraTS20_Training_{case}_flair.nii');\n    flair=nib.load(vol_path).get_fdata()\n    \n    vol_path = os.path.join(case_path, f'BraTS20_Training_{case}_t1ce.nii');\n    ce=nib.load(vol_path).get_fdata() \n    \n #   vol_path = os.path.join(case_path, f'BraTS20_Training_{case}_seg.nii');\n #   seg=nib.load(vol_path).get_fdata()  \n\n    \n    for j in range(VOLUME_SLICES):\n        X[j,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n        X[j,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n #       y[j,:,:] = cv2.resize(seg[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n        \n  #  model.evaluate(x=X,y=y[:,:,:,0], callbacks= callbacks)\n    return model.predict(X/np.max(X), verbose=1)\n\n\ndef showPredictsById(case, start_slice = 60):\n    path = f\"../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_{case}\"\n    gt = nib.load(os.path.join(path, f'BraTS20_Training_{case}_seg.nii')).get_fdata()\n    origImage = nib.load(os.path.join(path, f'BraTS20_Training_{case}_flair.nii')).get_fdata()\n    p = predictByPath(path,case)\n\n    core = p[:,:,:,1]\n    edema= p[:,:,:,2]\n    enhancing = p[:,:,:,3]\n\n    plt.figure(figsize=(18, 50))\n    f, axarr = plt.subplots(1,6, figsize = (18, 50)) \n\n    for i in range(6): # for each image, add brain background\n        axarr[i].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\", interpolation='none')\n    \n    axarr[0].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n    axarr[0].title.set_text('Original image flair')\n    curr_gt=cv2.resize(gt[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_NEAREST)\n    axarr[1].imshow(curr_gt, cmap=\"Reds\", interpolation='none', alpha=0.3) # ,alpha=0.3,cmap='Reds'\n    axarr[1].title.set_text('Ground truth')\n    axarr[2].imshow(p[start_slice,:,:,1:4], cmap=\"Reds\", interpolation='none', alpha=0.3)\n    axarr[2].title.set_text('all classes')\n    axarr[3].imshow(edema[start_slice,:,:], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    axarr[3].title.set_text(f'{SEGMENT_CLASSES[1]} predicted')\n    axarr[4].imshow(core[start_slice,:,], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    axarr[4].title.set_text(f'{SEGMENT_CLASSES[2]} predicted')\n    axarr[5].imshow(enhancing[start_slice,:,], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    axarr[5].title.set_text(f'{SEGMENT_CLASSES[3]} predicted')\n    plt.show()\n    \n    \nshowPredictsById(case=test_ids[0][-3:])\nshowPredictsById(case=test_ids[1][-3:])\nshowPredictsById(case=test_ids[2][-3:])\nshowPredictsById(case=test_ids[3][-3:])\nshowPredictsById(case=test_ids[4][-3:])\nshowPredictsById(case=test_ids[5][-3:])\nshowPredictsById(case=test_ids[6][-3:])\n\n\n# mask = np.zeros((10,10))\n# mask[3:-3, 3:-3] = 1 # white square in black background\n# im = mask + np.random.randn(10,10) * 0.01 # random image\n# masked = np.ma.masked_where(mask == 0, mask)\n\n# plt.figure()\n# plt.subplot(1,2,1)\n# plt.imshow(im, 'gray', interpolation='none')\n# plt.subplot(1,2,2)\n# plt.imshow(im, 'gray', interpolation='none')\n# plt.imshow(masked, 'jet', interpolation='none', alpha=0.7)\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:51:56.204633Z","iopub.execute_input":"2025-04-13T08:51:56.204888Z","iopub.status.idle":"2025-04-13T08:52:09.717783Z","shell.execute_reply.started":"2025-04-13T08:51:56.204864Z","shell.execute_reply":"2025-04-13T08:52:09.716952Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"case = case=test_ids[3][-3:]\npath = f\"../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_{case}\"\ngt = nib.load(os.path.join(path, f'BraTS20_Training_{case}_seg.nii')).get_fdata()\np = predictByPath(path,case)\n\n\ncore = p[:,:,:,1]\nedema= p[:,:,:,2]\nenhancing = p[:,:,:,3]\n\n\ni=40 # slice at\neval_class = 2 #     0 : 'NOT tumor',  1 : 'ENHANCING',    2 : 'CORE',    3 : 'WHOLE'\n\n\n\ngt[gt != eval_class] = 1 # use only one class for per class evaluation \n\nresized_gt = cv2.resize(gt[:,:,i+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE))\n\nplt.figure()\nf, axarr = plt.subplots(1,2) \naxarr[0].imshow(resized_gt, cmap=\"gray\")\naxarr[0].title.set_text('ground truth')\naxarr[1].imshow(p[i,:,:,eval_class], cmap=\"gray\")\naxarr[1].title.set_text(f'predicted class: {SEGMENT_CLASSES[eval_class]}')\nplt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:52:09.719370Z","iopub.execute_input":"2025-04-13T08:52:09.719714Z","iopub.status.idle":"2025-04-13T08:52:10.280429Z","shell.execute_reply.started":"2025-04-13T08:52:09.719676Z","shell.execute_reply":"2025-04-13T08:52:10.279803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics = ['accuracy',tf.keras.metrics.MeanIoU(num_classes=4), dice_coef, precision, sensitivity, specificity, dice_coef_necrotic, dice_coef_edema, dice_coef_enhancing] )\n# Evaluate the model on the test data using `evaluate`\nprint(\"Evaluate on test data\")\nresults = model.evaluate(test_generator, batch_size=100, callbacks= callbacks)\nprint(\"test loss, test acc:\", results)","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:52:10.281471Z","iopub.execute_input":"2025-04-13T08:52:10.281734Z","iopub.status.idle":"2025-04-13T08:52:47.623559Z","shell.execute_reply.started":"2025-04-13T08:52:10.281712Z","shell.execute_reply":"2025-04-13T08:52:47.622842Z"}},"outputs":[],"execution_count":null}]}